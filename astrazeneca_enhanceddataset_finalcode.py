# -*- coding: utf-8 -*-
"""AstraZeneca_EnhancedDataset_FinalCode.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dgHTi-CynFwoDTWDo39OAVbwI_vWLKFy
"""

!pip install rdkit

import os
import math
import csv
import numpy as np
import pandas as pd
from rdkit import Chem
from rdkit.Chem import rdchem
from rdkit import RDLogger
RDLogger.DisableLog("rdApp.*")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import (
    r2_score, mean_squared_error, mean_absolute_error,
    accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score
)
from scipy.stats import pearsonr, spearmanr
import matplotlib.pyplot as plt
from google.colab import drive
drive.mount("/content/drive")
summary_path = "/content/drive/MyDrive/Updated_Astrazeneca/all_synergy_summary.csv"
perconc_path = "/content/drive/MyDrive/Updated_Astrazeneca/all_synergy_per_concentration.csv"
drug_info_path = "/content/drive/MyDrive/Updated_Astrazeneca/Drug_info_release.csv"
output_dataset_path = "FinalEnhanced_UpdatedAZ_Dataset.csv"
RESULTS_DIR = "/content/drive/MyDrive/SynerGene_results"
FIG_DIR = os.path.join(RESULTS_DIR, "figures")
LOG_DIR = os.path.join(RESULTS_DIR, "logs")
os.makedirs(FIG_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
atom_types = [1, 6, 7, 8, 9, 15, 16, 17, 35, 53]
hyb_types = [rdchem.HybridizationType.SP,
             rdchem.HybridizationType.SP2,
             rdchem.HybridizationType.SP3]

def atom_features(atom):
    v = []
    z = atom.GetAtomicNum()
    v.extend([1.0 if z == t else 0.0 for t in atom_types])
    v.append(1.0 if z not in atom_types else 0.0)
    deg = atom.GetDegree()
    v.append(float(deg) / 4.0)
    formal = atom.GetFormalCharge()
    v.append(float(formal) / 3.0)
    hyb = atom.GetHybridization()
    v.extend([1.0 if hyb == h else 0.0 for h in hyb_types])
    v.append(1.0 if hyb not in hyb_types else 0.0)
    v.append(1.0 if atom.GetIsAromatic() else 0.0)
    v.append(float(atom.GetTotalNumHs()) / 4.0)
    return np.array(v, dtype=np.float32)
def smiles_to_graph(smiles, max_nodes=60):
    if not isinstance(smiles, str) or len(smiles.strip()) == 0:
        smiles = "C"
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        mol = Chem.MolFromSmiles("C")
    n = mol.GetNumAtoms()
    n_clamped = min(n, max_nodes)
    feat_list = []
    for i in range(n_clamped):
        atom = mol.GetAtomWithIdx(i)
        feat_list.append(atom_features(atom))
    if n_clamped == 0:
        feat = atom_features(Chem.MolFromSmiles("C").GetAtomWithIdx(0))
        feat_list = [feat]
        n_clamped = 1
    if n_clamped < max_nodes:
        pad = max_nodes - n_clamped
        feat_list.extend([np.zeros_like(feat_list[0]) for _ in range(pad)])
    x = np.stack(feat_list, axis=0)
    adj = np.zeros((max_nodes, max_nodes), dtype=np.float32)
    for bond in mol.GetBonds():
        i = bond.GetBeginAtomIdx()
        j = bond.GetEndAtomIdx()
        if i < max_nodes and j < max_nodes:
            adj[i, j] = 1.0
            adj[j, i] = 1.0
    for i in range(max_nodes):
        adj[i, i] = 1.0
    deg = adj.sum(-1, keepdims=True) + 1e-6
    adj_norm = adj / deg
    return x, adj_norm
def clean_and_validate_smiles(raw):
    if not isinstance(raw, str):
        return None
    s = raw.strip()
    if len(s) == 0:
        return None
    if s.isdigit():
        return None
    if ";" in s:
        s = s.split(";")[0].strip()
    mol = Chem.MolFromSmiles(s)
    if mol is None:
        return None
    return s
class SynergyDataset(Dataset):
    def __init__(self, df_summary, df_perconc, df_druginfo,
                 target_col="ZIP_Synergy_Score",
                 synergy_perconc_col="ZIP_synergy",
                 max_nodes=60, max_seq_len=36):
        self.max_nodes = max_nodes
        self.max_seq_len = max_seq_len

        df_summary = df_summary.dropna(
            subset=["Drug_A", "Drug_B", "Cell_Line", target_col]
        )
        df_perconc = df_perconc.dropna(
            subset=["Drug_A", "Drug_B", "Cell_Line",
                    "concentration_a", "concentration_b", synergy_perconc_col]
        )
        df_druginfo = df_druginfo.copy()
        df_druginfo["name_upper"] = df_druginfo["ChallengeName"].astype(str).str.upper()
        self.name_to_smiles = {}
        for _, row in df_druginfo.iterrows():
            name = str(row["ChallengeName"]).upper()
            raw = row["SMILES or PubChem ID"]
            sm = clean_and_validate_smiles(raw)
            if sm is not None:
                self.name_to_smiles[name] = sm

        group_cols = ["Drug_A", "Drug_B", "Cell_Line", "Replicates"]
        df_perconc = df_perconc[group_cols +
                                ["concentration_a", "concentration_b", synergy_perconc_col]]
        self.seq_dict = {}
        for key, sub in df_perconc.groupby(group_cols):
            sub_sorted = sub.sort_values(["concentration_a", "concentration_b"])
            seq = sub_sorted[["concentration_a", "concentration_b",
                              synergy_perconc_col]].values.astype(np.float32)
            self.seq_dict[key] = seq

        cell_list = sorted(df_summary["Cell_Line"].unique())
        self.cell_to_idx = {c: i for i, c in enumerate(cell_list)}

        xA_list, adjA_list = [], []
        xB_list, adjB_list = [], []
        cell_idx_list = []
        seq_list, seq_mask_list = [], []
        y_list = []
        drugA_names, drugB_names, cell_names, rep_ids = [], [], [], []

        for _, row in df_summary.iterrows():
            dA = str(row["Drug_A"]).upper()
            dB = str(row["Drug_B"]).upper()
            cl = row["Cell_Line"]
            rep = row.get("Replicates", "Rep1")
            key = (row["Drug_A"], row["Drug_B"], row["Cell_Line"], rep)
            if key not in self.seq_dict:
                continue
            seq = self.seq_dict[key]
            if dA in self.name_to_smiles:
                sA = self.name_to_smiles[dA]
            else:
                sA = "C"
            if dB in self.name_to_smiles:
                sB = self.name_to_smiles[dB]
            else:
                sB = "C"

            xA_np, adjA_np = smiles_to_graph(sA, max_nodes=self.max_nodes)
            xB_np, adjB_np = smiles_to_graph(sB, max_nodes=self.max_nodes)

            if cl not in self.cell_to_idx:
                continue

            y_val = float(row[target_col])

            xA_list.append(xA_np)
            adjA_list.append(adjA_np)
            xB_list.append(xB_np)
            adjB_list.append(adjB_np)
            cell_idx_list.append(self.cell_to_idx[cl])
            y_list.append(y_val)
            drugA_names.append(row["Drug_A"])
            drugB_names.append(row["Drug_B"])
            cell_names.append(cl)
            rep_ids.append(rep)
            seq_len = seq.shape[0]
            if seq_len >= self.max_seq_len:
                seq_trunc = seq[: self.max_seq_len]
                mask = np.ones(self.max_seq_len, dtype=np.float32)
            else:
                pad_len = self.max_seq_len - seq_len
                pad = np.zeros((pad_len, seq.shape[1]), dtype=np.float32)
                seq_trunc = np.concatenate([seq, pad], axis=0)
                mask = np.concatenate([np.ones(seq_len, dtype=np.float32),
                                       np.zeros(pad_len, dtype=np.float32)], axis=0)
            seq_list.append(seq_trunc)
            seq_mask_list.append(mask)

        self.xA = torch.tensor(np.stack(xA_list), dtype=torch.float32)
        self.adjA = torch.tensor(np.stack(adjA_list), dtype=torch.float32)
        self.xB = torch.tensor(np.stack(xB_list), dtype=torch.float32)
        self.adjB = torch.tensor(np.stack(adjB_list), dtype=torch.float32)
        self.seq = torch.tensor(np.stack(seq_list), dtype=torch.float32)
        self.seq_mask = torch.tensor(np.stack(seq_mask_list), dtype=torch.float32)
        self.cell_idx = torch.tensor(np.array(cell_idx_list), dtype=torch.long)
        y_raw_np = np.array(y_list, dtype=np.float32)
        y_clipped = np.clip(y_raw_np, -10.0, 10.0)
        self.y = torch.tensor(y_clipped, dtype=torch.float32)
        self.mean_y = float(y_clipped.mean())
        self.std_y = float(y_clipped.std() + 1e-6)
        y_scaled_np = (y_clipped - self.mean_y) / self.std_y
        self.y_scaled = torch.tensor(y_scaled_np, dtype=torch.float32)
        cls_np = np.zeros_like(y_clipped, dtype=np.int64)
        cls_np[y_clipped <= -10.0] = 0
        cls_np[(y_clipped > -10.0) & (y_clipped < 10.0)] = 1
        cls_np[y_clipped >= 10.0] = 2
        self.y_class = torch.tensor(cls_np, dtype=torch.long)
        self.drugA = drugA_names
        self.drugB = drugB_names
        self.cells = cell_names
        self.reps = rep_ids
        self.n_cells = len(self.cell_to_idx)
        self.n_samples = self.y.shape[0]
        self.n_features = self.xA.shape[-1]
        self.seq_len = self.seq.shape[1]

    def __len__(self):
        return self.n_samples

    def __getitem__(self, idx):
        return (self.xA[idx], self.adjA[idx],
                self.xB[idx], self.adjB[idx],
                self.cell_idx[idx],
                self.seq[idx], self.seq_mask[idx],
                self.y_scaled[idx], self.y[idx],
                self.y_class[idx],
                self.drugA[idx], self.drugB[idx], self.cells[idx], self.reps[idx])

class GCNLayer(nn.Module):
    def __init__(self, in_dim, out_dim):
        super().__init__()
        self.lin = nn.Linear(in_dim, out_dim)

    def forward(self, x, adj):
        h = torch.matmul(adj, x)
        h = self.lin(h)
        return F.relu(h)

class GINLayer(nn.Module):
    def __init__(self, in_dim, out_dim, eps=0.0):
        super().__init__()
        self.eps = nn.Parameter(torch.tensor(eps, dtype=torch.float32))
        self.mlp = nn.Sequential(
            nn.Linear(in_dim, out_dim), nn.ReLU(),
            nn.Linear(out_dim, out_dim)
        )

    def forward(self, x, adj):
        agg = torch.matmul(adj, x)
        h = (1 + self.eps) * x + agg
        h = self.mlp(h)
        return h

class GATLayer(nn.Module):
    def __init__(self, in_dim, out_dim):
        super().__init__()
        self.lin = nn.Linear(in_dim, out_dim, bias=False)
        self.a = nn.Linear(2 * out_dim, 1, bias=False)

    def forward(self, x, adj):
        h = self.lin(x)
        B, N, Fh = h.shape
        h_i = h.unsqueeze(2).expand(B, N, N, Fh)
        h_j = h.unsqueeze(1).expand(B, N, N, Fh)
        cat = torch.cat([h_i, h_j], dim=-1)
        e = self.a(cat).squeeze(-1)
        e = e.masked_fill(adj == 0, -1e9)
        alpha = F.softmax(e, dim=-1)
        out = torch.matmul(alpha, h)
        return F.elu(out)

class MPMLayer(nn.Module):
    def __init__(self, in_dim, out_dim):
        super().__init__()
        self.lin_msg = nn.Linear(in_dim, out_dim)
        self.lin_upd = nn.Linear(in_dim + out_dim, out_dim)

    def forward(self, x, adj):
        m = torch.matmul(adj, self.lin_msg(x))
        h = torch.cat([x, m], dim=-1)
        h = F.relu(self.lin_upd(h))
        return h

class GlobalAttentionPool(nn.Module):
    def __init__(self, in_dim, attn_dim):
        super().__init__()
        self.w = nn.Linear(in_dim, attn_dim)
        self.u = nn.Linear(attn_dim, 1)

    def forward(self, x, mask=None):
        e = self.u(torch.tanh(self.w(x))).squeeze(-1)
        if mask is not None:
            e = e.masked_fill(mask == 0, -1e9)
        alpha = F.softmax(e, dim=-1)
        out = torch.sum(alpha.unsqueeze(-1) * x, dim=1)
        return out

class GraphEncoder(nn.Module):
    def __init__(self, in_dim, emb_dim, backbone, attn_dim):
        super().__init__()
        if backbone == "gcn":
            self.conv1 = GCNLayer(in_dim, emb_dim)
            self.conv2 = GCNLayer(emb_dim, emb_dim)
        elif backbone == "gin":
            self.conv1 = GINLayer(in_dim, emb_dim)
            self.conv2 = GINLayer(emb_dim, emb_dim)
        elif backbone == "gat":
            self.conv1 = GATLayer(in_dim, emb_dim)
            self.conv2 = GATLayer(emb_dim, emb_dim)
        elif backbone == "mpnn":
            self.conv1 = MPMLayer(in_dim, emb_dim)
            self.conv2 = MPMLayer(emb_dim, emb_dim)
        else:
            raise ValueError("Unknown backbone:", backbone)
        self.attn_pool = GlobalAttentionPool(emb_dim, attn_dim)

    def forward(self, x, adj):
        h = self.conv1(x, adj)
        h = self.conv2(h, adj)
        out = self.attn_pool(h, None)
        return out

class SeqEncoder(nn.Module):
    def __init__(self, input_dim, hidden_dim, attn_dim, dropout=0.3):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)
        self.attn = GlobalAttentionPool(2 * hidden_dim, attn_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, seq, mask):
        h, _ = self.lstm(seq)
        h = self.dropout(h)
        out = self.attn(h, mask)
        return out

class SynerGeneModel(nn.Module):
    def __init__(self, in_dim, emb_dim, cell_emb_dim,
                 seq_hidden_dim, backbone,
                 attn_dim=64, hidden_dim=256,
                 n_cells=1, n_classes=3, dropout=0.3):
        super().__init__()
        self.encoder_A = GraphEncoder(in_dim, emb_dim, backbone, attn_dim)
        self.encoder_B = GraphEncoder(in_dim, emb_dim, backbone, attn_dim)
        self.cell_emb = nn.Embedding(n_cells, cell_emb_dim)
        self.seq_encoder = SeqEncoder(input_dim=3, hidden_dim=seq_hidden_dim,
                                      attn_dim=attn_dim, dropout=dropout)
        fused_dim = emb_dim * 2 + cell_emb_dim + 2 * seq_hidden_dim
        self.fc1 = nn.Linear(fused_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.dropout = nn.Dropout(dropout)
        self.out_reg = nn.Linear(hidden_dim, 1)
        self.out_cls = nn.Linear(hidden_dim, n_classes)

    def forward(self, xA, adjA, xB, adjB, cell_idx, seq, seq_mask):
        gA = self.encoder_A(xA, adjA)
        gB = self.encoder_B(xB, adjB)
        c = self.cell_emb(cell_idx)
        seq_vec = self.seq_encoder(seq, seq_mask)
        z = torch.cat([gA, gB, c, seq_vec], dim=-1)
        z = F.relu(self.fc1(z))
        z = self.dropout(z)
        z = F.relu(self.fc2(z))
        z = self.dropout(z)
        y_reg = self.out_reg(z).squeeze(-1)
        y_cls_logits = self.out_cls(z)
        return y_reg, y_cls_logits

class SynergyPerDoseDataset(Dataset):
    def __init__(self, df_perconc, df_druginfo,
                 synergy_perconc_col="ZIP_synergy",
                 max_nodes=60):
        self.max_nodes = max_nodes

        df_perconc = df_perconc.dropna(
            subset=["Drug_A", "Drug_B", "Cell_Line",
                    "concentration_a", "concentration_b", synergy_perconc_col]
        ).copy()

        df_druginfo = df_druginfo.copy()
        df_druginfo["name_upper"] = df_druginfo["ChallengeName"].astype(str).str.upper()
        self.name_to_smiles = {}
        for _, row in df_druginfo.iterrows():
            name = str(row["ChallengeName"]).upper()
            raw = row["SMILES or PubChem ID"]
            sm = clean_and_validate_smiles(raw)
            if sm is not None:
                self.name_to_smiles[name] = sm
        cell_list = sorted(df_perconc["Cell_Line"].astype(str).unique())
        self.cell_to_idx = {c: i for i, c in enumerate(cell_list)}
        xA_list, adjA_list = [], []
        xB_list, adjB_list = [], []
        cell_idx_list = []
        conc_list = []
        y_list = []
        drugA_names, drugB_names, cell_names = [], [], []

        for _, row in df_perconc.iterrows():
            dA = str(row["Drug_A"]).upper()
            dB = str(row["Drug_B"]).upper()
            cl = str(row["Cell_Line"])
            if dA in self.name_to_smiles:
                sA = self.name_to_smiles[dA]
            else:
                sA = "C"
            if dB in self.name_to_smiles:
                sB = self.name_to_smiles[dB]
            else:
                sB = "C"

            xA_np, adjA_np = smiles_to_graph(sA, max_nodes=self.max_nodes)
            xB_np, adjB_np = smiles_to_graph(sB, max_nodes=self.max_nodes)
            cell_idx = self.cell_to_idx[cl]
            conc_a = float(row["concentration_a"])
            conc_b = float(row["concentration_b"])
            y_val = float(row[synergy_perconc_col])
            xA_list.append(xA_np)
            adjA_list.append(adjA_np)
            xB_list.append(xB_np)
            adjB_list.append(adjB_np)
            cell_idx_list.append(cell_idx)
            conc_list.append([conc_a, conc_b])
            y_list.append(y_val)
            drugA_names.append(str(row["Drug_A"]))
            drugB_names.append(str(row["Drug_B"]))
            cell_names.append(cl)

        self.xA = torch.tensor(np.stack(xA_list), dtype=torch.float32)
        self.adjA = torch.tensor(np.stack(adjA_list), dtype=torch.float32)
        self.xB = torch.tensor(np.stack(xB_list), dtype=torch.float32)
        self.adjB = torch.tensor(np.stack(adjB_list), dtype=torch.float32)
        self.cell_idx = torch.tensor(np.array(cell_idx_list, dtype=np.int64), dtype=torch.long)
        conc_np = np.array(conc_list, dtype=np.float32)
        self.conc_mean = conc_np.mean(axis=0)
        self.conc_std = conc_np.std(axis=0) + 1e-6
        conc_scaled = (conc_np - self.conc_mean) / self.conc_std
        self.conc = torch.tensor(conc_scaled, dtype=torch.float32)
        y_np = np.array(y_list, dtype=np.float32)
        y_np = np.clip(y_np, -30.0, 30.0)
        self.y = torch.tensor(y_np, dtype=torch.float32)
        self.mean_y = float(y_np.mean())
        self.std_y = float(y_np.std() + 1e-6)
        y_scaled_np = (y_np - self.mean_y) / self.std_y
        self.y_scaled = torch.tensor(y_scaled_np, dtype=torch.float32)
        cls_np = np.zeros_like(y_np, dtype=np.int64)
        cls_np[y_np < -10.0] = 0
        cls_np[(y_np >= -10.0) & (y_np <= 10.0)] = 1
        cls_np[y_np > 10.0] = 2
        self.y_class = torch.tensor(cls_np, dtype=torch.long)
        self.drugA = drugA_names
        self.drugB = drugB_names
        self.cells = cell_names
        self.n_cells = len(self.cell_to_idx)
        self.n_samples = self.y.shape[0]
        self.n_features = self.xA.shape[-1]

    def __len__(self):
        return self.n_samples

    def __getitem__(self, idx):
        return (self.xA[idx],
                self.adjA[idx],
                self.xB[idx],
                self.adjB[idx],
                self.cell_idx[idx],
                self.conc[idx],
                self.y_scaled[idx],
                self.y[idx],
                self.y_class[idx],
                self.drugA[idx],
                self.drugB[idx],
                self.cells[idx])

class SynerGenePerDoseModel(nn.Module):
    def __init__(self, in_dim, emb_dim, cell_emb_dim,
                 backbone, conc_hidden=32,
                 hidden_dim=256, attn_dim=64,
                 n_cells=1, n_classes=3, dropout=0.3):
        super().__init__()
        self.encoder_A = GraphEncoder(in_dim, emb_dim, backbone, attn_dim)
        self.encoder_B = GraphEncoder(in_dim, emb_dim, backbone, attn_dim)
        self.cell_emb = nn.Embedding(n_cells, cell_emb_dim)

        self.conc_mlp = nn.Sequential(
            nn.Linear(2, conc_hidden),
            nn.ReLU(),
            nn.Linear(conc_hidden, conc_hidden),
            nn.ReLU()
        )
        fused_dim = emb_dim * 2 + cell_emb_dim + conc_hidden
        self.fc1 = nn.Linear(fused_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.dropout = nn.Dropout(dropout)
        self.out_reg = nn.Linear(hidden_dim, 1)
        self.out_cls = nn.Linear(hidden_dim, n_classes)

    def forward(self, xA, adjA, xB, adjB, cell_idx, conc):
        gA = self.encoder_A(xA, adjA)
        gB = self.encoder_B(xB, adjB)
        c = self.cell_emb(cell_idx)
        conc_vec = self.conc_mlp(conc)
        z = torch.cat([gA, gB, c, conc_vec], dim=-1)
        z = F.relu(self.fc1(z))
        z = self.dropout(z)
        z = F.relu(self.fc2(z))
        z = self.dropout(z)
        y_reg = self.out_reg(z).squeeze(-1)
        y_cls_logits = self.out_cls(z)
        return y_reg, y_cls_logits

def compute_regression_metrics(y_true, y_pred):
    y_true_np = y_true.detach().cpu().numpy()
    y_pred_np = y_pred.detach().cpu().numpy()
    r2 = r2_score(y_true_np, y_pred_np)
    rmse = math.sqrt(mean_squared_error(y_true_np, y_pred_np))
    mae = mean_absolute_error(y_true_np, y_pred_np)
    try:
        pearson = pearsonr(y_true_np, y_pred_np)[0]
    except Exception:
        pearson = 0.0
    try:
        spearman = spearmanr(y_true_np, y_pred_np)[0]
    except Exception:
        spearman = 0.0
    return {"R2": r2, "RMSE": rmse, "MAE": mae, "Pearson": pearson, "Spearman": spearman}

def compute_classification_metrics(y_true_cls, y_prob, n_classes=3):
    y_true_np = y_true_cls.detach().cpu().numpy()
    y_prob_np = y_prob.detach().cpu().numpy()
    y_pred_np = y_prob_np.argmax(axis=1)
    acc = accuracy_score(y_true_np, y_pred_np)
    bacc = balanced_accuracy_score(y_true_np, y_pred_np)
    f1 = f1_score(y_true_np, y_pred_np, average="macro")
    try:
        if n_classes == 2:
            auc = roc_auc_score(y_true_np, y_prob_np[:, 1])
        else:
            auc = roc_auc_score(y_true_np, y_prob_np, multi_class="ovr")
    except Exception:
        auc = 0.0
    return {"ACC": acc, "BACC": bacc, "F1": f1, "AUC": auc}

def train_one_model(backbone, dataset,
                    batch_size=64, lr=1e-4,
                    weight_decay=2e-3, epochs=30,
                    train_idx=None, val_idx=None):

    if train_idx is None or val_idx is None:
        idx = np.arange(len(dataset))
        idx_train, idx_val = train_test_split(
            idx,
            test_size=0.2,
            random_state=42,
            stratify=dataset.y_class.numpy()
        )
    else:
        idx_train = np.array(train_idx)
        idx_val = np.array(val_idx)

    class_counts = np.bincount(dataset.y_class.numpy(), minlength=3)
    class_weights = 1.0 / (class_counts + 1e-6)
    class_weights = class_weights / class_weights.sum()
    class_weights_t = torch.tensor(class_weights, dtype=torch.float32).to(device)

    y_train_cls = dataset.y_class.numpy()[idx_train]
    classes = np.unique(y_train_cls)
    min_count = min(np.sum(y_train_cls == c) for c in classes)

    balanced_idx_train = []
    for c in classes:
        idx_c = idx_train[y_train_cls == c]
        if len(idx_c) > min_count:
            idx_c = np.random.choice(idx_c, size=min_count, replace=False)
        balanced_idx_train.append(idx_c)
    balanced_idx_train = np.concatenate(balanced_idx_train)
    np.random.shuffle(balanced_idx_train)
    train_subset = torch.utils.data.Subset(dataset, balanced_idx_train)
    val_subset = torch.utils.data.Subset(dataset, idx_val)
    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)

    model = SynerGeneModel(
        in_dim=dataset.n_features,
        emb_dim=64,
        cell_emb_dim=64,
        seq_hidden_dim=64,
        backbone=backbone,
        n_cells=dataset.n_cells
    ).to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    criterion_cls = nn.CrossEntropyLoss(weight=class_weights_t)
    reg_history = {"R2": [], "RMSE": [], "MAE": [], "Pearson": [], "Spearman": []}
    cls_history = {"ACC": [], "BACC": [], "F1": [], "AUC": []}
    best_r2 = -1e9
    best_state = None

    for epoch in range(epochs):
        model.train()
        for (xA, adjA, xB, adjB, cell_idx, seq, seq_mask,
             y_scaled, y_raw, y_cls, dA, dB, cl, rep) in train_loader:
            xA = xA.to(device)
            adjA = adjA.to(device)
            xB = xB.to(device)
            adjB = adjB.to(device)
            cell_idx = cell_idx.to(device)
            seq = seq.to(device)
            seq_mask = seq_mask.to(device)
            y_scaled = y_scaled.to(device)
            y_cls = y_cls.to(device)

            optimizer.zero_grad()
            y_reg_scaled, y_logits = model(xA, adjA, xB, adjB,
                                           cell_idx, seq, seq_mask)
            loss_reg = F.mse_loss(y_reg_scaled, y_scaled)
            loss_cls = criterion_cls(y_logits, y_cls)
            loss = loss_reg + 0.5 * loss_cls
            loss.backward()
            optimizer.step()

        model.eval()
        y_true_list, y_pred_list = [], []
        y_true_cls_list, y_prob_list = [], []

        with torch.no_grad():
            for (xA, adjA, xB, adjB, cell_idx, seq, seq_mask,
                 y_scaled, y_raw, y_cls, dA, dB, cl, rep) in val_loader:
                xA = xA.to(device)
                adjA = adjA.to(device)
                xB = xB.to(device)
                adjB = adjB.to(device)
                cell_idx = cell_idx.to(device)
                seq = seq.to(device)
                seq_mask = seq_mask.to(device)
                y_raw = y_raw.to(device)
                y_cls = y_cls.to(device)

                y_reg_scaled, y_logits = model(xA, adjA, xB, adjB,
                                               cell_idx, seq, seq_mask)
                y_reg = y_reg_scaled * dataset.std_y + dataset.mean_y

                y_true_list.append(y_raw)
                y_pred_list.append(y_reg)
                y_true_cls_list.append(y_cls)
                y_prob_list.append(F.softmax(y_logits, dim=-1))

        y_true = torch.cat(y_true_list, dim=0)
        y_pred = torch.cat(y_pred_list, dim=0)
        y_true_cls = torch.cat(y_true_cls_list, dim=0)
        y_prob = torch.cat(y_prob_list, dim=0)

        reg = compute_regression_metrics(y_true, y_pred)
        cls = compute_classification_metrics(y_true_cls, y_prob, n_classes=3)

        for k in reg_history.keys():
            reg_history[k].append(reg[k])
        for k in cls_history.keys():
            cls_history[k].append(cls[k])

        if reg["R2"] > best_r2:
            best_r2 = reg["R2"]
            best_state = model.state_dict()
            print(
                backbone,
                "| Epoch", epoch + 1, "/", epochs,
                "| R2", reg["R2"],
                "| MAE", reg["MAE"],
                "| Pearson", reg["Pearson"],
                "| Spearman", reg["Spearman"],
                "| BACC", cls["BACC"],
                "| AUC", cls["AUC"]
            )

    if best_state is not None:
        model.load_state_dict(best_state)

    return model, reg_history, cls_history

def train_one_model_perdose(backbone, dataset,
                            batch_size=64, lr=1e-4,
                            weight_decay=2e-3, epochs=30):
    class_counts = np.bincount(dataset.y_class.numpy(), minlength=3)
    class_weights = 1.0 / (class_counts + 1e-6)
    class_weights = class_weights / class_weights.sum()
    class_weights_t = torch.tensor(class_weights, dtype=torch.float32).to(device)

    idx = np.arange(len(dataset))
    idx_train, idx_val = train_test_split(
        idx,
        test_size=0.2,
        random_state=42,
        stratify=dataset.y_class.numpy()
    )

    y_train_cls = dataset.y_class.numpy()[idx_train]
    classes = np.unique(y_train_cls)
    min_count = min(np.sum(y_train_cls == c) for c in classes)

    balanced_idx_train = []
    for c in classes:
        idx_c = idx_train[y_train_cls == c]
        if len(idx_c) > min_count:
            idx_c = np.random.choice(idx_c, size=min_count, replace=False)
        balanced_idx_train.append(idx_c)
    balanced_idx_train = np.concatenate(balanced_idx_train)
    np.random.shuffle(balanced_idx_train)

    train_subset = torch.utils.data.Subset(dataset, balanced_idx_train)
    val_subset = torch.utils.data.Subset(dataset, idx_val)

    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)

    model = SynerGenePerDoseModel(
        in_dim=dataset.n_features,
        emb_dim=64,
        cell_emb_dim=64,
        backbone=backbone,
        n_cells=dataset.n_cells
    ).to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    criterion_cls = nn.CrossEntropyLoss(weight=class_weights_t)
    reg_history = {"R2": [], "RMSE": [], "MAE": [], "Pearson": [], "Spearman": []}
    cls_history = {"ACC": [], "BACC": [], "F1": [], "AUC": []}
    best_r2 = -1e9
    best_state = None

    for epoch in range(epochs):
        model.train()
        for (xA, adjA, xB, adjB, cell_idx, conc,
             y_scaled, y_raw, y_cls, dA, dB, cl) in train_loader:
            xA = xA.to(device)
            adjA = adjA.to(device)
            xB = xB.to(device)
            adjB = adjB.to(device)
            cell_idx = cell_idx.to(device)
            conc = conc.to(device)
            y_scaled = y_scaled.to(device)
            y_cls = y_cls.to(device)

            optimizer.zero_grad()
            y_reg_scaled, y_logits = model(xA, adjA, xB, adjB, cell_idx, conc)
            loss_reg = F.mse_loss(y_reg_scaled, y_scaled)
            loss_cls = criterion_cls(y_logits, y_cls)
            loss = loss_reg + 0.5 * loss_cls
            loss.backward()
            optimizer.step()

        model.eval()
        y_true_list, y_pred_list = [], []
        y_true_cls_list, y_prob_list = [], []
        with torch.no_grad():
            for (xA, adjA, xB, adjB, cell_idx, conc,
                 y_scaled, y_raw, y_cls, dA, dB, cl) in val_loader:
                xA = xA.to(device)
                adjA = adjA.to(device)
                xB = xB.to(device)
                adjB = adjB.to(device)
                cell_idx = cell_idx.to(device)
                conc = conc.to(device)
                y_raw = y_raw.to(device)
                y_cls = y_cls.to(device)

                y_reg_scaled, y_logits = model(xA, adjA, xB, adjB, cell_idx, conc)
                y_reg = y_reg_scaled * dataset.std_y + dataset.mean_y

                y_true_list.append(y_raw)
                y_pred_list.append(y_reg)
                y_true_cls_list.append(y_cls)
                y_prob_list.append(F.softmax(y_logits, dim=-1))

        y_true = torch.cat(y_true_list, dim=0)
        y_pred = torch.cat(y_pred_list, dim=0)
        y_true_cls = torch.cat(y_true_cls_list, dim=0)
        y_prob = torch.cat(y_prob_list, dim=0)

        reg = compute_regression_metrics(y_true, y_pred)
        cls = compute_classification_metrics(y_true_cls, y_prob, n_classes=3)

        for k in reg_history.keys():
            reg_history[k].append(reg[k])
        for k in cls_history.keys():
            cls_history[k].append(cls[k])

        if reg["R2"] > best_r2:
            best_r2 = reg["R2"]
            best_state = model.state_dict()
            print(
                backbone,
                "| Epoch", epoch + 1, "/", epochs,
                "| R2", reg["R2"],
                "| MAE", reg["MAE"],
                "| Pearson", reg["Pearson"],
                "| Spearman", reg["Spearman"],
                "| ACC", cls["ACC"],
                "| BACC", cls["BACC"],
                "| F1", cls["F1"],
                "| AUC", cls["AUC"]
            )

    if best_state is not None:
        model.load_state_dict(best_state)

    return model, reg_history, cls_history

def save_histories_to_csv(backbone, fold_id, reg_history, cls_history, log_dir=LOG_DIR):
    filename = f"{backbone}_fold{fold_id}_history.csv"
    path = os.path.join(log_dir, filename)

    epochs = len(next(iter(reg_history.values())))
    fieldnames = [
        "epoch",
        "R2", "Pearson", "Spearman", "MAE", "RMSE",
        "ACC", "BACC", "F1", "AUC"
    ]

    with open(path, "w", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for e in range(epochs):
            row = {"epoch": e + 1}
            for k in reg_history:
                row[k] = reg_history[k][e]
            for k in cls_history:
                row[k] = cls_history[k][e]
            writer.writerow(row)

def plot_metric_histories(histories_reg, histories_cls, epochs, backbones):
    metrics_reg = ["R2", "Pearson", "Spearman", "MAE", "RMSE"]
    metrics_cls = ["ACC", "BACC", "F1", "AUC"]

    for m in metrics_reg:
        plt.figure()
        for b in backbones:
            plt.plot(range(1, epochs + 1), histories_reg[b][m], label=b)
        plt.xlabel("Epoch")
        plt.ylabel(m)
        plt.title("Regression " + m + " per epoch")
        plt.legend()
        plt.tight_layout()
        fname = f"reg_{m}_per_epoch.png"
        plt.savefig(os.path.join(FIG_DIR, fname))
        plt.show()
        plt.close()

    for m in metrics_cls:
        plt.figure()
        for b in backbones:
            plt.plot(range(1, epochs + 1), histories_cls[b][m], label=b)
        plt.xlabel("Epoch")
        plt.ylabel(m)
        plt.title("Classification " + m + " per epoch")
        plt.legend()
        plt.tight_layout()
        fname = f"cls_{m}_per_epoch.png"
        plt.savefig(os.path.join(FIG_DIR, fname))
        plt.show()
        plt.close()

    final_reg = {b: {m: histories_reg[b][m][-1] for m in metrics_reg} for b in backbones}
    final_cls = {b: {m: histories_cls[b][m][-1] for m in metrics_cls} for b in backbones}
    df_reg = pd.DataFrame(final_reg).T
    df_cls = pd.DataFrame(final_cls).T

    plt.figure()
    df_reg.plot(kind="bar")
    plt.title("Final regression metrics per model")
    plt.xticks(rotation=0)
    plt.tight_layout()
    fname = "final_regression_metrics.png"
    plt.savefig(os.path.join(FIG_DIR, fname))
    plt.show()
    plt.close()
    plt.figure()
    df_cls.plot(kind="bar")
    plt.title("Final classification metrics per model")
    plt.xticks(rotation=0)
    plt.tight_layout()
    fname = "final_classification_metrics.png"
    plt.savefig(os.path.join(FIG_DIR, fname))
    plt.show()
    plt.close()
    corr_reg = df_reg.corr()
    corr_cls = df_cls.corr()
    plt.figure()
    plt.imshow(corr_reg.values, aspect="auto")
    plt.colorbar()
    plt.xticks(range(len(corr_reg.columns)), corr_reg.columns, rotation=45)
    plt.yticks(range(len(corr_reg.index)), corr_reg.index)
    plt.title("Regression metrics correlation heatmap")
    plt.tight_layout()
    fname = "regression_metrics_correlation.png"
    plt.savefig(os.path.join(FIG_DIR, fname))
    plt.show()
    plt.close()
    plt.figure()
    plt.imshow(corr_cls.values, aspect="auto")
    plt.colorbar()
    plt.xticks(range(len(corr_cls.columns)), corr_cls.columns, rotation=45)
    plt.yticks(range(len(corr_cls.index)), corr_cls.index)
    plt.title("Classification metrics correlation heatmap")
    plt.tight_layout()
    fname = "classification_metrics_correlation.png"
    plt.savefig(os.path.join(FIG_DIR, fname))
    plt.show()
    plt.close()
    return df_reg, df_cls

def build_processed_dataset(models, dataset, backbones):
    xA = dataset.xA.to(device)
    adjA = dataset.adjA.to(device)
    xB = dataset.xB.to(device)
    adjB = dataset.adjB.to(device)
    seq = dataset.seq.to(device)
    seq_mask = dataset.seq_mask.to(device)
    cell_idx = dataset.cell_idx.to(device)
    y_true = dataset.y.numpy()
    y_class = dataset.y_class.numpy()
    rows = []
    for b in backbones:
        model = models[b]
        model.eval()
        with torch.no_grad():
            y_scaled, y_logits = model(xA, adjA, xB, adjB, cell_idx, seq, seq_mask)
            y_pred = (y_scaled * dataset.std_y + dataset.mean_y).cpu().numpy()
            cls_prob = F.softmax(y_logits, dim=-1).cpu().numpy()
            cls_pred = cls_prob.argmax(axis=1)
        for i in range(len(y_true)):
            rows.append({
                "backbone": b,
                "index": int(i),
                "Drug_A": dataset.drugA[i],
                "Drug_B": dataset.drugB[i],
                "Cell_Line": dataset.cells[i],
                "Replicates": dataset.reps[i],
                "true_synergy": float(y_true[i]),
                "true_class": int(y_class[i]),
                "pred_synergy": float(y_pred[i]),
                "pred_class": int(cls_pred[i]),
                "prob_class_0": float(cls_prob[i, 0]),
                "prob_class_1": float(cls_prob[i, 1]),
                "prob_class_2": float(cls_prob[i, 2])
            })
    df_out = pd.DataFrame(rows)
    df_out.to_csv(output_dataset_path, index=False)
    return df_out

df_summary = pd.read_csv(summary_path)
df_perconc = pd.read_csv(perconc_path)
df_info = pd.read_csv(drug_info_path)

dataset = SynergyDataset(
    df_summary,
    df_perconc,
    df_info,
    target_col="ZIP_Synergy_Score",
    synergy_perconc_col="ZIP_synergy",
    max_nodes=60,
    max_seq_len=36
)
print("Summary y min/max:", dataset.y.min().item(), dataset.y.max().item())
print("Summary class counts:", np.unique(dataset.y_class.numpy(), return_counts=True))

perdose_dataset = SynergyPerDoseDataset(
    df_perconc,
    df_info,
    synergy_perconc_col="ZIP_synergy",
    max_nodes=60
)
print("Per-dose class counts:", np.bincount(perdose_dataset.y_class.numpy()))

epochs = 30

model_zip_perdose, reg_zip_perdose, cls_zip_perdose = train_one_model_perdose(
    "gcn",
    perdose_dataset,
    epochs=epochs
)

def visualize_pair(df_perconc, drugA_name, drugB_name, cell_line,
                   conc_a, conc_b):
    sub = df_perconc[
        (df_perconc["Drug_A"] == drugA_name) &
        (df_perconc["Drug_B"] == drugB_name) &
        (df_perconc["Cell_Line"] == cell_line)
    ]
    if sub.empty:
        print("No per-concentration data found for this combination")
        return None, None

    sub = sub.copy()
    sub["dist"] = (sub["concentration_a"] - conc_a) ** 2 + \
                  (sub["concentration_b"] - conc_b) ** 2
    idx_min = sub["dist"].idxmin()
    chosen = sub.loc[idx_min]
    val_point = float(chosen["ZIP_synergy"])

    min_zip = float(sub["ZIP_synergy"].min())
    max_zip = float(sub["ZIP_synergy"].max())
    perc_synergy = 100.0 * (val_point - min_zip) / (max_zip - min_zip + 1e-6)

    pivot = sub.pivot_table(index="concentration_a",
                            columns="concentration_b",
                            values="ZIP_synergy",
                            aggfunc="mean")
    pivot = pivot.sort_index(axis=0).sort_index(axis=1)

    plt.figure()
    plt.imshow(pivot.values, origin="lower", aspect="auto")
    plt.colorbar(label="ZIP_synergy")
    plt.xticks(range(len(pivot.columns)),
               [str(c) for c in pivot.columns],
               rotation=45)
    plt.yticks(range(len(pivot.index)),
               [str(r) for r in pivot.index])
    plt.xlabel("concentration_b")
    plt.ylabel("concentration_a")
    plt.title(f"ZIP_synergy heatmap: {drugA_name} + {drugB_name} on {cell_line}")
    plt.tight_layout()
    plt.show()

    sub_sorted = sub.sort_values(
        ["concentration_a", "concentration_b"]
    ).reset_index(drop=True)
    x_idx = np.arange(len(sub_sorted))
    y_vals = sub_sorted["ZIP_synergy"].values

    chosen_idx = sub_sorted.index[sub_sorted["dist"].idxmin()]
    colors = ["yellow" if i == chosen_idx else "blue"
              for i in range(len(sub_sorted))]

    plt.figure()
    plt.plot(x_idx, y_vals)
    plt.scatter(x_idx, y_vals, c=colors)
    plt.xlabel("Dose index (sorted by concentrations)")
    plt.ylabel("ZIP_synergy")
    plt.title("ZIP_synergy across doses (selected dose in yellow)")
    plt.tight_layout()
    plt.show()

    return val_point, perc_synergy

def predict_zip_per_concentration(
    model,
    dataset,
    drugA_name,
    drugB_name,
    cell_line,
    conc_a,
    conc_b,
    device=device
):
    model.eval()

    dA = str(drugA_name).upper().strip()
    dB = str(drugB_name).upper().strip()
    cl = str(cell_line).strip()

    if cl not in dataset.cell_to_idx:
        raise ValueError(f"Unknown cell line: {cell_line}")
    cell_idx_val = dataset.cell_to_idx[cl]

    if hasattr(dataset, "name_to_smiles"):
        if dA in dataset.name_to_smiles:
            sA = dataset.name_to_smiles[dA]
        else:
            sA = "C"
        if dB in dataset.name_to_smiles:
            sB = dataset.name_to_smiles[dB]
        else:
            sB = "C"
    else:
        sA = "C"
        sB = "C"

    xA_np, adjA_np = smiles_to_graph(sA, max_nodes=dataset.max_nodes)
    xB_np, adjB_np = smiles_to_graph(sB, max_nodes=dataset.max_nodes)

    xA = torch.tensor(xA_np, dtype=torch.float32).unsqueeze(0).to(device)
    adjA = torch.tensor(adjA_np, dtype=torch.float32).unsqueeze(0).to(device)
    xB = torch.tensor(xB_np, dtype=torch.float32).unsqueeze(0).to(device)
    adjB = torch.tensor(adjB_np, dtype=torch.float32).unsqueeze(0).to(device)
    cell_idx = torch.tensor([cell_idx_val], dtype=torch.long).to(device)

    conc_np = np.array([[float(conc_a), float(conc_b)]], dtype=np.float32)
    conc_scaled = (conc_np - dataset.conc_mean) / dataset.conc_std
    conc = torch.tensor(conc_scaled, dtype=torch.float32).to(device)

    with torch.no_grad():
        out = model(xA, adjA, xB, adjB, cell_idx, conc)

        if isinstance(out, (tuple, list)):
            y_reg_scaled = out[0]
        else:
            y_reg_scaled = out

        y_reg = y_reg_scaled * dataset.std_y + dataset.mean_y
        y_pred_val = float(y_reg.cpu().numpy().reshape(-1)[0])

    if y_pred_val < -10:
        pred_label = "Antagonistic"
    elif y_pred_val > 10:
        pred_label = "Synergistic"
    else:
        pred_label = "Additive"

    mask = [
        (da.upper() == dA and db.upper() == dB and c == cl)
        for da, db, c in zip(dataset.drugA, dataset.drugB, dataset.cells)
    ]
    true_vals = [float(dataset.y[i].item()) for i, m in enumerate(mask) if m]

    return {
        "drugA": drugA_name,
        "drugB": drugB_name,
        "cell_line": cell_line,
        "conc_a": conc_a,
        "conc_b": conc_b,
        "pred_zip": y_pred_val,
        "pred_label_from_zip": pred_label,
        "matching_true_zip_values_in_dataset": true_vals
    }

def test_per_concentration(
    model,
    perdose_dataset,
    df_perconc,
    drugA_name,
    drugB_name,
    cell_line,
    conc_a,
    conc_b
):
    res = predict_zip_per_concentration(
        model,
        perdose_dataset,
        drugA_name,
        drugB_name,
        cell_line,
        conc_a,
        conc_b,
        device=device
    )

    val_point, perc_synergy = visualize_pair(
        df_perconc,
        drugA_name,
        drugB_name,
        cell_line,
        conc_a,
        conc_b
    )

    print(f"Drug A: {drugA_name} | Drug B: {drugB_name} | Cell line: {cell_line}")
    print(f"Requested conc_a: {conc_a} | conc_b: {conc_b}")

    print("\n[MODEL PREDICTION]")
    print("Predicted ZIP_synergy at this concentration:", res["pred_zip"])
    print("Predicted label from ZIP thresholds:", res["pred_label_from_zip"])

    print("\n[DATASET ACTUAL (nearest grid point)]")
    if val_point is not None:
        print("Actual ZIP_synergy at closest grid dose:", val_point)
        print("Relative synergy position in grid (%):", perc_synergy)
    else:
        print("No per-concentration grid found for this combination.")

    if res["matching_true_zip_values_in_dataset"]:
        print("\nAll matching ZIP values in perdose_dataset for this combo:",
              res["matching_true_zip_values_in_dataset"])
    else:
        print("\nNo exact (DrugA, DrugB, Cell) combo rows in perdose_dataset matched.")


drugA_test = "ADAM17"
drugB_test = "AKT"
cell_test  = "BT-20"
conc_a_test = 0.01
conc_b_test = 0.75

test_per_concentration(
    model_zip_perdose,
    perdose_dataset,
    df_perconc,
    drugA_test,
    drugB_test,
    cell_test,
    conc_a_test,
    conc_b_test
)