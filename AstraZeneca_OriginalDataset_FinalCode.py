# -*- coding: utf-8 -*-
"""Assumable_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YsgnYr8GcK-c0ZJDCwTp_TRQAz_jbweX
"""

!pip install rdkit
import os
import math
import random
import numpy as np
import pandas as pd
from rdkit import Chem
from rdkit.Chem import rdchem
from rdkit import RDLogger
RDLogger.DisableLog("rdApp.*")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from scipy.stats import pearsonr, spearmanr
import matplotlib.pyplot as plt
from google.colab import drive
drive.mount("/content/drive")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

MAIN_FILE = "/content/drive/MyDrive/AstraZeneca_ALLFILES_NEW/Astrazeneca_Main.xlsx"
DRUG_PATHWAY_FILE = "/content/drive/MyDrive/AstraZeneca_ALLFILES_NEW/Drugname_Pathway.xlsx"
CELLLINE_FILE = "/content/drive/MyDrive/AstraZeneca_ALLFILES_NEW/CellLineName.xlsx"
MUT_FILE = "/content/drive/MyDrive/AstraZeneca_ALLFILES_NEW/All_mutations.xlsx"
SMILES_FILE = "/content/drive/MyDrive/AstraZeneca_ALLFILES_NEW/drug_smiles.csv"
DRUG_INFO_FILE = "/content/drive/MyDrive/AstraZeneca_ALLFILES_NEW/Drug_info_release.csv"

atom_types = [1, 6, 7, 8, 9, 15, 16, 17, 35, 53]
hyb_types = [rdchem.HybridizationType.SP,
             rdchem.HybridizationType.SP2,
             rdchem.HybridizationType.SP3]

def atom_features(atom):
    v = []
    z = atom.GetAtomicNum()
    v.extend([1.0 if z == t else 0.0 for t in atom_types])
    v.append(1.0 if z not in atom_types else 0.0)
    deg = atom.GetDegree()
    v.append(float(deg) / 4.0)
    formal = atom.GetFormalCharge()
    v.append(float(formal) / 3.0)
    hyb = atom.GetHybridization()
    v.extend([1.0 if hyb == h else 0.0 for h in hyb_types])
    v.append(1.0 if hyb not in hyb_types else 0.0)
    v.append(1.0 if atom.GetIsAromatic() else 0.0)
    v.append(float(atom.GetTotalNumHs()) / 4.0)
    return np.array(v, dtype=np.float32)

def smiles_to_graph(smiles, max_nodes=60):
    if not isinstance(smiles, str) or len(smiles.strip()) == 0:
        smiles = "C"
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        mol = Chem.MolFromSmiles("C")
    n = mol.GetNumAtoms()
    n_clamped = min(n, max_nodes)
    feat_list = []
    for i in range(n_clamped):
        atom = mol.GetAtomWithIdx(i)
        feat_list.append(atom_features(atom))

    if n_clamped == 0:
        feat = atom_features(Chem.MolFromSmiles("C").GetAtomWithIdx(0))
        feat_list = [feat]
        n_clamped = 1

    if n_clamped < max_nodes:
        pad = max_nodes - n_clamped
        feat_list.extend([np.zeros_like(feat_list[0]) for _ in range(pad)])

    x = np.stack(feat_list, axis=0)
    adj = np.zeros((max_nodes, max_nodes), dtype=np.float32)
    for bond in mol.GetBonds():
        i = bond.GetBeginAtomIdx()
        j = bond.GetEndAtomIdx()
        if i < max_nodes and j < max_nodes:
            adj[i, j] = 1.0
            adj[j, i] = 1.0
    for i in range(max_nodes):
        adj[i, i] = 1.0

    deg = adj.sum(-1, keepdims=True) + 1e-6
    adj_norm = adj / deg
    return x, adj_norm

def clean_and_validate_smiles(raw):
    if not isinstance(raw, str):
        return None
    s = raw.strip()
    if len(s) == 0:
        return None
    if s.isdigit():
        return None
    if ";" in s:
        s = s.split(";")[0].strip()
    mol = Chem.MolFromSmiles(s)
    if mol is None:
        return None
    return s

import re
df_main      = pd.read_excel(MAIN_FILE)
df_pathway   = pd.read_excel(DRUG_PATHWAY_FILE)
df_cellinfo  = pd.read_excel(CELLLINE_FILE)
df_mut       = pd.read_excel(MUT_FILE)
df_smiles    = pd.read_csv(SMILES_FILE)
df_druginfo  = pd.read_csv(DRUG_INFO_FILE)
print("Main dataset shape:", df_main.shape)
df_pathway = df_pathway.copy()
df_pathway["Drug_name_upper"]      = df_pathway["Drug name"].astype(str).str.upper()
df_pathway["Challenge_upper"]      = df_pathway["Challenge drug name"].astype(str).str.upper()
drug_to_challenge = dict(
    zip(df_pathway["Drug_name_upper"], df_pathway["Challenge_upper"])
)

df_druginfo = df_druginfo.copy()
df_druginfo["Challenge_upper"] = df_druginfo["ChallengeName"].astype(str).str.upper()
challenge_to_smiles = {}
for _, row in df_druginfo.iterrows():
    cname = str(row["ChallengeName"]).upper()
    raw_sm = row["SMILES or PubChem ID"]
    sm = clean_and_validate_smiles(raw_sm)
    if sm is not None:
        challenge_to_smiles[cname] = sm

df_smiles = df_smiles.copy()
df_smiles["Drug_upper"] = df_smiles["Drug"].astype(str).str.upper()
drugname_to_smiles = {}
for _, row in df_smiles.iterrows():
    dname = str(row["Drug"]).upper()
    raw = row["SMILES"]
    sm = clean_and_validate_smiles(raw)
    if sm is not None:
        drugname_to_smiles[dname] = sm

print("len(challenge_to_smiles):", len(challenge_to_smiles))
print("len(drugname_to_smiles):", len(drugname_to_smiles))
def normalize_compound_name(raw_name: str) -> str:
    if not isinstance(raw_name, str):
        return ""
    s = raw_name.strip()
    if len(s) == 0:
        return ""
    parts = re.split(r"[,/;]", s)
    parts = [p.strip() for p in parts if len(p.strip()) > 0]
    if len(parts) == 0:
        return s.strip().upper()
    return parts[0].upper()

def get_smiles_for_compound(comp_name: str) -> str:

    key = normalize_compound_name(comp_name)
    if key == "":
        return None

    if key in drugname_to_smiles:
        return drugname_to_smiles[key]

    if key in drug_to_challenge:
        chall = drug_to_challenge[key]
        if chall in challenge_to_smiles:
            return challenge_to_smiles[chall]

    if key in challenge_to_smiles:
        return challenge_to_smiles[key]

    return None

for i in range(5):
    ca = df_main.loc[i, "COMPOUND_A"]
    cb = df_main.loc[i, "COMPOUND_B"]
    print("Row", i, "| A:", ca, "->", get_smiles_for_compound(ca),
          "| B:", cb, "->", get_smiles_for_compound(cb))

class AZMainDataset(Dataset):
    def __init__(
        self,
        df_main,
        max_nodes=60,
        max_seq_len=1,
        target_col="SYNERGY_SCORE"
    ):
        self.max_nodes = max_nodes
        self.max_seq_len = max_seq_len

        df = df_main.copy()

        df = df.dropna(subset=["CELL_LINE", "COMPOUND_A", "COMPOUND_B", target_col,
                               "IC50_A", "IC50_B"])

        df = df[(df[target_col] > -250.0) & (df[target_col] < 250.0)]

        ic50_a_mean = df["IC50_A"].mean()
        ic50_a_std  = df["IC50_A"].std() + 1e-6
        ic50_b_mean = df["IC50_B"].mean()
        ic50_b_std  = df["IC50_B"].std() + 1e-6

        self.ic50_a_mean = float(ic50_a_mean)
        self.ic50_a_std  = float(ic50_a_std)
        self.ic50_b_mean = float(ic50_b_mean)
        self.ic50_b_std  = float(ic50_b_std)

        cell_list = sorted(df["CELL_LINE"].unique())
        self.cell_to_idx = {c: i for i, c in enumerate(cell_list)}

        xA_list, adjA_list = [], []
        xB_list, adjB_list = [], []
        cell_idx_list = []
        seq_list, seq_mask_list = [], []
        y_list = []

        for _, row in df.iterrows():
            compA = row["COMPOUND_A"]
            compB = row["COMPOUND_B"]
            cell  = row["CELL_LINE"]
            y_val = float(row[target_col])

            if cell not in self.cell_to_idx:
                continue

            smA = get_smiles_for_compound(compA)
            smB = get_smiles_for_compound(compB)

            if smA is None:
                smA = "C"
            if smB is None:
                smB = "C"

            xA_np, adjA_np = smiles_to_graph(smA, max_nodes=self.max_nodes)
            xB_np, adjB_np = smiles_to_graph(smB, max_nodes=self.max_nodes)

            ic50_a = (float(row["IC50_A"]) - self.ic50_a_mean) / self.ic50_a_std
            ic50_b = (float(row["IC50_B"]) - self.ic50_b_mean) / self.ic50_b_std

            seq = np.zeros((self.max_seq_len, 2), dtype=np.float32)
            seq[0, 0] = ic50_a
            seq[0, 1] = ic50_b
            mask = np.zeros(self.max_seq_len, dtype=np.float32)
            mask[0] = 1.0

            xA_list.append(xA_np)
            adjA_list.append(adjA_np)
            xB_list.append(xB_np)
            adjB_list.append(adjB_np)
            cell_idx_list.append(self.cell_to_idx[cell])
            y_list.append(y_val)
            seq_list.append(seq)
            seq_mask_list.append(mask)

        self.xA = torch.tensor(np.stack(xA_list), dtype=torch.float32)
        self.adjA = torch.tensor(np.stack(adjA_list), dtype=torch.float32)
        self.xB = torch.tensor(np.stack(xB_list), dtype=torch.float32)
        self.adjB = torch.tensor(np.stack(adjB_list), dtype=torch.float32)
        self.cell_idx = torch.tensor(np.array(cell_idx_list), dtype=torch.long)
        self.seq = torch.tensor(np.stack(seq_list), dtype=torch.float32)
        self.seq_mask = torch.tensor(np.stack(seq_mask_list), dtype=torch.float32)

        y_raw_np = np.array(y_list, dtype=np.float32)
        self.y_raw = torch.tensor(y_raw_np, dtype=torch.float32)

        y_for_scale = y_raw_np
        self.mean_y = float(y_for_scale.mean())
        self.std_y  = float(y_for_scale.std() + 1e-6)
        y_scaled_np = (y_raw_np - self.mean_y) / self.std_y
        self.y_scaled = torch.tensor(y_scaled_np, dtype=torch.float32)

        self.n_samples = self.y_raw.shape[0]
        self.n_features = self.xA.shape[-1]
        self.n_cells = len(self.cell_to_idx)
        self.seq_len = self.seq.shape[1]

        print("AZMainDataset initialized:")
        print("  n_samples:", self.n_samples)
        print("  n_cells:", self.n_cells)
        print("  n_features (atom):", self.n_features)
        print("  seq_len:", self.seq_len)

    def __len__(self):
        return self.n_samples

    def __getitem__(self, idx):
        return (
            self.xA[idx],
            self.adjA[idx],
            self.xB[idx],
            self.adjB[idx],
            self.cell_idx[idx],
            self.seq[idx],
            self.seq_mask[idx],
            self.y_scaled[idx],
            self.y_raw[idx]
        )

az_dataset = AZMainDataset(df_main)

class GCNLayer(nn.Module):
    def __init__(self, in_dim, out_dim):
        super().__init__()
        self.lin = nn.Linear(in_dim, out_dim)

    def forward(self, x, adj):
        h = torch.matmul(adj, x)
        h = self.lin(h)
        return F.relu(h)

class GINLayer(nn.Module):
    def __init__(self, in_dim, out_dim, eps=0.0):
        super().__init__()
        self.eps = nn.Parameter(torch.tensor(eps, dtype=torch.float32))
        self.mlp = nn.Sequential(
            nn.Linear(in_dim, out_dim),
            nn.ReLU(),
            nn.Linear(out_dim, out_dim)
        )

    def forward(self, x, adj):
        agg = torch.matmul(adj, x)
        h = (1 + self.eps) * x + agg
        h = self.mlp(h)
        return h

class GATLayer(nn.Module):
    def __init__(self, in_dim, out_dim):
        super().__init__()
        self.lin = nn.Linear(in_dim, out_dim, bias=False)
        self.a   = nn.Linear(2 * out_dim, 1, bias=False)

    def forward(self, x, adj):
        h = self.lin(x)
        B, N, Fh = h.shape
        h_i = h.unsqueeze(2).expand(B, N, N, Fh)
        h_j = h.unsqueeze(1).expand(B, N, N, Fh)
        cat = torch.cat([h_i, h_j], dim=-1)
        e   = self.a(cat).squeeze(-1)
        e   = e.masked_fill(adj == 0, -1e9)
        alpha = F.softmax(e, dim=-1)
        out   = torch.matmul(alpha, h)
        return F.elu(out)

class MPMLayer(nn.Module):
    def __init__(self, in_dim, out_dim):
        super().__init__()
        self.lin_msg = nn.Linear(in_dim, out_dim)
        self.lin_upd = nn.Linear(in_dim + out_dim, out_dim)

    def forward(self, x, adj):
        m = torch.matmul(adj, self.lin_msg(x))
        h = torch.cat([x, m], dim=-1)
        h = F.relu(self.lin_upd(h))
        return h

class GlobalAttention(nn.Module):
    def __init__(self, in_dim, attn_dim):
        super().__init__()
        self.w = nn.Linear(in_dim, attn_dim)
        self.u = nn.Linear(attn_dim, 1)

    def forward(self, x, mask=None):
        # x: (B, N, F)
        e = self.u(torch.tanh(self.w(x))).squeeze(-1)
        if mask is not None:
            e = e.masked_fill(mask == 0, -1e9)
        alpha = F.softmax(e, dim=-1)
        out = torch.sum(alpha.unsqueeze(-1) * x, dim=1)
        return out

class GraphEncoder(nn.Module):
    def __init__(self, in_dim, emb_dim, backbone, attn_dim):
        super().__init__()
        if backbone == "gcn":
            self.conv1 = GCNLayer(in_dim, emb_dim)
            self.conv2 = GCNLayer(emb_dim, emb_dim)
        elif backbone == "gin":
            self.conv1 = GINLayer(in_dim, emb_dim)
            self.conv2 = GINLayer(emb_dim, emb_dim)
        elif backbone == "gat":
            self.conv1 = GATLayer(in_dim, emb_dim)
            self.conv2 = GATLayer(emb_dim, emb_dim)
        elif backbone == "mpnn":
            self.conv1 = MPMLayer(in_dim, emb_dim)
            self.conv2 = MPMLayer(emb_dim, emb_dim)
        else:
            raise ValueError(f"Unknown backbone: {backbone}")

        self.attn_pool = GlobalAttention(emb_dim, attn_dim)

    def forward(self, x, adj):
        h = self.conv1(x, adj)
        h = self.conv2(h, adj)
        out = self.attn_pool(h, None)
        return out

class SeqEncoder(nn.Module):
    def __init__(self, input_dim, hidden_dim, attn_dim, dropout=0.3):
        super().__init__()
        self.lstm   = nn.LSTM(input_dim, hidden_dim, batch_first=True,
                              bidirectional=True)
        self.attn   = GlobalAttention(2 * hidden_dim, attn_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, seq, mask):
        h, _ = self.lstm(seq)
        h = self.dropout(h)
        out = self.attn(h, mask)
        return out

class SynerGeneModel(nn.Module):
    def __init__(
        self,
        in_dim,
        emb_dim,
        cell_emb_dim,
        seq_hidden_dim,
        backbone,
        attn_dim=64,
        hidden_dim=256,
        n_cells=1,
        dropout=0.10
    ):
        super().__init__()
        self.encoder_A = GraphEncoder(in_dim, emb_dim, backbone, attn_dim)
        self.encoder_B = GraphEncoder(in_dim, emb_dim, backbone, attn_dim)
        self.cell_emb  = nn.Embedding(n_cells, cell_emb_dim)
        self.seq_encoder = SeqEncoder(input_dim=2,
                                      hidden_dim=seq_hidden_dim,
                                      attn_dim=attn_dim,
                                      dropout=dropout)

        fused_dim = emb_dim * 2 + cell_emb_dim + 2 * seq_hidden_dim
        self.fc1 = nn.Linear(fused_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.dropout = nn.Dropout(dropout)
        self.out_reg = nn.Linear(hidden_dim, 1)

    def forward(self, xA, adjA, xB, adjB, cell_idx, seq, seq_mask):
        gA = self.encoder_A(xA, adjA)
        gB = self.encoder_B(xB, adjB)
        c  = self.cell_emb(cell_idx)
        s  = self.seq_encoder(seq, seq_mask)
        z  = torch.cat([gA, gB, c, s], dim=-1)
        z  = F.relu(self.fc1(z))
        z  = self.dropout(z)
        z  = F.relu(self.fc2(z))
        z  = self.dropout(z)
        y_reg = self.out_reg(z).squeeze(-1)
        return y_reg

def compute_regression_metrics(y_true, y_pred):
    y_true_np = y_true.detach().cpu().numpy()
    y_pred_np = y_pred.detach().cpu().numpy()

    r2   = r2_score(y_true_np, y_pred_np)
    rmse = math.sqrt(mean_squared_error(y_true_np, y_pred_np))
    mae  = mean_absolute_error(y_true_np, y_pred_np)

    try:
        pearson = pearsonr(y_true_np, y_pred_np)[0]
    except Exception:
        pearson = 0.0
    try:
        spearman = spearmanr(y_true_np, y_pred_np)[0]
    except Exception:
        spearman = 0.0

    return {
        "R2": r2,
        "RMSE": rmse,
        "MAE": mae,
        "Pearson": pearson,
        "Spearman": spearman
    }

from sklearn.model_selection import KFold

def pearson_loss(y_true, y_pred, eps=1e-8):
    x = y_true - y_true.mean()
    y = y_pred - y_pred.mean()
    vx = torch.sum(x * x)
    vy = torch.sum(y * y)
    denom = torch.sqrt(vx * vy) + eps
    corr = torch.sum(x * y) / denom
    return 1.0 - corr


def train_one_model(
    backbone,
    dataset,
    batch_size=128,
    lr=3e-4,
    weight_decay=1e-6,
    epochs=30,
    n_folds=5,
    patience=20,
    corr_alpha=0.6
):
    indices = np.arange(len(dataset))
    kf = KFold(n_splits=n_folds, shuffle=True, random_state=123)

    all_fold_histories = []
    best_global_r2 = -1e9
    best_global_spearman = -1e9
    best_global_state = None

    for fold, (idx_train, idx_val) in enumerate(kf.split(indices)):
        print(f"\n========== {backbone.upper()} | FOLD {fold+1}/{n_folds} ==========")

        train_subset = torch.utils.data.Subset(dataset, idx_train)
        val_subset   = torch.utils.data.Subset(dataset, idx_val)

        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)
        val_loader   = DataLoader(val_subset,   batch_size=batch_size, shuffle=False)

        model = SynerGeneModel(
            in_dim=dataset.n_features,
            emb_dim=128,
            cell_emb_dim=128,
            seq_hidden_dim=128,
            backbone=backbone,
            attn_dim=96,
            hidden_dim=640,
            n_cells=dataset.n_cells,
            dropout=0.25
        ).to(device)

        optimizer = torch.optim.Adam(
            model.parameters(), lr=lr, weight_decay=weight_decay
        )
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode="max",
            factor=0.5,
            patience=6
        )

        criterion_reg = nn.MSELoss()

        history = {
            "R2":        [],
            "RMSE":      [],
            "MAE":       [],
            "Pearson":   [],
            "Spearman":  [],
            "TrainLoss": [],
            "ValLoss":   []
        }

        best_r2_fold = -1e9
        best_spearman_fold = -1e9
        best_state_fold = None
        no_improve = 0

        for epoch in range(epochs):
            model.train()
            train_loss_sum = 0.0
            train_n = 0

            for (xA, adjA, xB, adjB, cell_idx, seq, seq_mask,
                 y_scaled, y_raw) in train_loader:

                xA       = xA.to(device)
                adjA     = adjA.to(device)
                xB       = xB.to(device)
                adjB     = adjB.to(device)
                cell_idx = cell_idx.to(device)
                seq      = seq.to(device)
                seq_mask = seq_mask.to(device)
                y_scaled = y_scaled.to(device)

                optimizer.zero_grad()
                y_pred_scaled = model(xA, adjA, xB, adjB, cell_idx, seq, seq_mask)

                base_loss = criterion_reg(y_pred_scaled, y_scaled)
                corr_term = pearson_loss(y_scaled, y_pred_scaled)
                loss = (1.0 - corr_alpha) * base_loss + corr_alpha * corr_term

                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)
                optimizer.step()

                bs = xA.size(0)
                train_loss_sum += loss.item() * bs
                train_n        += bs

            train_loss_epoch = train_loss_sum / max(train_n, 1)

            model.eval()
            val_loss_sum = 0.0
            val_n = 0
            y_true_list = []
            y_pred_list = []

            with torch.no_grad():
                for (xA, adjA, xB, adjB, cell_idx, seq, seq_mask,
                     y_scaled, y_raw) in val_loader:

                    xA       = xA.to(device)
                    adjA     = adjA.to(device)
                    xB       = xB.to(device)
                    adjB     = adjB.to(device)
                    cell_idx = cell_idx.to(device)
                    seq      = seq.to(device)
                    seq_mask = seq_mask.to(device)
                    y_scaled = y_scaled.to(device)
                    y_raw    = y_raw.to(device)

                    y_pred_scaled = model(xA, adjA, xB, adjB,
                                          cell_idx, seq, seq_mask)

                    base_loss = criterion_reg(y_pred_scaled, y_scaled)
                    corr_term = pearson_loss(y_scaled, y_pred_scaled)
                    loss = (1.0 - corr_alpha) * base_loss + corr_alpha * corr_term

                    bs = xA.size(0)
                    val_loss_sum += loss.item() * bs
                    val_n        += bs

                    y_pred = y_pred_scaled * dataset.std_y + dataset.mean_y

                    y_true_list.append(y_raw)
                    y_pred_list.append(y_pred)

            val_loss_epoch = val_loss_sum / max(val_n, 1)

            y_true = torch.cat(y_true_list, dim=0)
            y_pred = torch.cat(y_pred_list, dim=0)
            reg = compute_regression_metrics(y_true, y_pred)

            scheduler.step(reg["Pearson"])

            history["R2"].append(reg["R2"])
            history["RMSE"].append(reg["RMSE"])
            history["MAE"].append(reg["MAE"])
            history["Pearson"].append(reg["Pearson"])
            history["Spearman"].append(reg["Spearman"])
            history["TrainLoss"].append(train_loss_epoch)
            history["ValLoss"].append(val_loss_epoch)

            improved = False
            if reg["R2"] > best_r2_fold:
                best_r2_fold = reg["R2"]
                improved = True
            if reg["Spearman"] > best_spearman_fold:
                best_spearman_fold = reg["Spearman"]
                best_state_fold = model.state_dict()
                improved = True

            if improved:
                no_improve = 0
            else:
                no_improve += 1

            print(
                f"{backbone.upper()} | Fold {fold+1}/{n_folds} | "
                f"Epoch {epoch+1}/{epochs} "
                f"| R2={reg['R2']:.4f} "
                f"| RMSE={reg['RMSE']:.4f} "
                f"| MAE={reg['MAE']:.4f} "
                f"| Pearson={reg['Pearson']:.4f} "
                f"| Spearman={reg['Spearman']:.4f} "
                f"| TrainLoss={train_loss_epoch:.4f} "
                f"| ValLoss={val_loss_epoch:.4f}"
            )

            if no_improve >= patience:
                print(f"Early stopping on fold {fold+1} at epoch {epoch+1}")
                break

        print(f"Best R2 for {backbone.upper()} on FOLD {fold+1}: {best_r2_fold:.4f}")
        print(f"Best Spearman for {backbone.upper()} on FOLD {fold+1}: {best_spearman_fold:.4f}")

        if best_r2_fold > best_global_r2:
            best_global_r2 = best_r2_fold
        if best_spearman_fold > best_global_spearman:
            best_global_spearman = best_spearman_fold
            best_global_state = best_state_fold

        all_fold_histories.append(history)

    avg_history = {}
    keys = all_fold_histories[0].keys()
    max_len = max(len(h["R2"]) for h in all_fold_histories)
    for k in keys:
        avg_vals = []
        for ep in range(max_len):
            vals = []
            for fh in all_fold_histories:
                if ep < len(fh[k]):
                    vals.append(fh[k][ep])
            if len(vals) > 0:
                avg_vals.append(float(np.mean(vals)))
        avg_history[k] = avg_vals

    final_model = SynerGeneModel(
        in_dim=dataset.n_features,
        emb_dim=128,
        cell_emb_dim=128,
        seq_hidden_dim=128,
        backbone=backbone,
        attn_dim=96,
        hidden_dim=640,
        n_cells=dataset.n_cells,
        dropout=0.25
    ).to(device)

    if best_global_state is not None:
        final_model.load_state_dict(best_global_state)

    print(f"\n***** {backbone.upper()} | BEST CV R2 across folds: {best_global_r2:.4f} *****")
    print(f"***** {backbone.upper()} | BEST CV Spearman across folds: {best_global_spearman:.4f} *****")

    return final_model, avg_history

gcn_model, gcn_history = train_one_model(
    backbone="gcn",
    dataset=az_dataset,
    batch_size=256,
    lr=3e-4,
    weight_decay=1e-6,
    epochs=30,
    n_folds=5,
    patience=15,
    corr_alpha=0.6
)

gin_model, gin_history = train_one_model(
    backbone="gin",
    dataset=az_dataset,
    batch_size=256,
    lr=3e-4,
    weight_decay=1e-6,
    epochs=30,
    n_folds=5,
    patience=15,
    corr_alpha=0.6
)

gat_model, gat_history = train_one_model(
    backbone="gat",
    dataset=az_dataset,
    batch_size=256,
    lr=3e-4,
    weight_decay=1e-4,
    epochs=30,
    n_folds=5,
    patience=15,
    corr_alpha=0.6
)

mpnn_model, mpnn_history = train_one_model(
    backbone="mpnn",
    dataset=az_dataset,
    batch_size=128,
    lr=3e-4,
    weight_decay=1e-4,
    epochs=30,
    n_folds=5,
    patience=15,
    corr_alpha=0.6
)

histories = {
    "GCN":  gcn_history,
    "GIN":  gin_history,
    "GAT":  gat_history,
    "MPNN": mpnn_history
}

metrics = ["R2", "RMSE", "MAE", "Pearson", "Spearman"]
loss_metrics = ["TrainLoss", "ValLoss"]

epochs = range(1, len(next(iter(histories.values()))["R2"]) + 1)

for m in metrics:
    plt.figure(figsize=(7, 5))
    for name, hist in histories.items():
        plt.plot(epochs, hist[m], label=name)
    plt.xlabel("Epoch")
    plt.ylabel(m)
    plt.title(f"{m} over epochs for each backbone")
    plt.legend()
    plt.grid(True)
    plt.show()

for m in loss_metrics:
    plt.figure(figsize=(7, 5))
    for name, hist in histories.items():
        plt.plot(epochs, hist[m], label=name)
    plt.xlabel("Epoch")
    plt.ylabel(m)
    plt.title(f"{m} over epochs for each backbone")
    plt.legend()
    plt.grid(True)
    plt.show()

import numpy as np

for m in metrics:
    best_vals = []
    labels = []
    for name, hist in histories.items():
        if m == "R2":
            vals = hist[m]
            best_val = max(vals)
        else:
            vals = hist[m]
            if m in ["RMSE", "MAE"]:
                best_val = min(vals)
            else:
                best_val = max(vals)
        best_vals.append(best_val)
        labels.append(name)

    x = np.arange(len(labels))
    plt.figure(figsize=(7, 5))
    plt.bar(x, best_vals)
    plt.xticks(x, labels)
    plt.ylabel(m)
    plt.title(f"Best {m} across backbones")
    plt.grid(axis="y")
    plt.show()

import re
import matplotlib.pyplot as plt

gat_R2, gat_RMSE, gat_MAE, gat_Pearson, gat_Spearman = [], [], [], [], []

for line in log_text.splitlines():
    if "GAT | Fold 1/5" in line and "| Epoch" in line:
        m = re.search(
            r"R2=([0-9.\-eE]+)\s*\|\s*RMSE=([0-9.\-eE]+)\s*\|\s*MAE=([0-9.\-eE]+)\s*\|\s*Pearson=([0-9.\-eE]+)\s*\|\s*Spearman=([0-9.\-eE]+)",
            line
        )
        if m:
            r2, rmse, mae, pear, spear = map(float, m.groups())
            gat_R2.append(r2)
            gat_RMSE.append(rmse)
            gat_MAE.append(mae)
            gat_Pearson.append(pear)
            gat_Spearman.append(spear)

gat_history = {
    "R2": gat_R2,
    "RMSE": gat_RMSE,
    "MAE": gat_MAE,
    "Pearson": gat_Pearson,
    "Spearman": gat_Spearman,
}

print(f"GAT epochs parsed: {len(gat_R2)}")

reg_histories = {
    "gcn":  gcn_history,
    "gin":  gin_history,
    "gat":  gat_history,
    "mpnn": mpnn_history,
}

metrics = {
    "R2":       ("Regression R² per epoch", "R²"),
    "RMSE":     ("Regression RMSE per epoch", "RMSE"),
    "MAE":      ("Regression MAE per epoch", "MAE"),
    "Pearson":  ("Regression Pearson r per epoch", "Pearson r"),
    "Spearman": ("Regression Spearman ρ per epoch", "Spearman ρ"),
}

for metric, (title, ylabel) in metrics.items():
    plt.figure(figsize=(6, 4))
    epochs = range(1, len(next(iter(reg_histories.values()))[metric]) + 1)
    for name, hist in reg_histories.items():
        plt.plot(epochs, hist[metric], label=name)
    plt.title(title)
    plt.xlabel("Epoch")
    plt.ylabel(ylabel)
    plt.legend()
    plt.tight_layout()
    plt.show()

import numpy as np
import matplotlib.pyplot as plt

# ===========================
# 1) FINAL METRICS YOU ALREADY HAVE
# ===========================
final_metrics = {
    "gcn":  {"R2":0.3115, "RMSE":28.5661, "MAE":21.1628, "Pearson":0.5611, "Spearman":0.3883},
    "gin":  {"R2":0.2859, "RMSE":29.0913, "MAE":21.6594, "Pearson":0.5421, "Spearman":0.3968},
    "gat":  {"R2":0.2163, "RMSE":34.0794, "MAE":23.9121, "Pearson":0.3660, "Spearman":0.3066},
    "mpnn": {"R2":0.2957, "RMSE":28.8917, "MAE":21.6569, "Pearson":0.5439, "Spearman":0.3641},
}

models = list(final_metrics.keys())
metrics = ["R2", "Pearson", "Spearman", "MAE", "RMSE"]
M = np.array([[final_metrics[m][k] for k in metrics] for m in models])
corr = np.corrcoef(M.T)
plt.figure(figsize=(8, 6))
plt.imshow(corr, cmap="Blues", vmin=-1, vmax=1)
plt.xticks(range(len(metrics)), metrics)
plt.yticks(range(len(metrics)), metrics)
cbar = plt.colorbar()
cbar.set_label("Correlation")
plt.title("Regression metrics correlation heatmap")

for i in range(len(metrics)):
    for j in range(len(metrics)):
        plt.text(j, i, f"{corr[i, j]:.2f}",
                 ha="center", va="center", color="black")

plt.tight_layout()
plt.show()
plt.figure(figsize=(10, 6))
x = np.arange(len(models))
width = 0.15
for i, metric in enumerate(metrics):
    vals = [final_metrics[m][metric] for m in models]
    plt.bar(x + i * width, vals, width=width, label=metric)

plt.xticks(x + width * 2, models)
plt.ylabel("Metric Value")
plt.title("Final regression metrics per model")
plt.legend()
plt.tight_layout()
plt.show()

